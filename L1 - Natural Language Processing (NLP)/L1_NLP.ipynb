{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "In the example provided, the libraries are already installed. If we wanted to run this, we would have to download the model libraries that we want from Hugging Face & run with the Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the chatbot pipeline with Hugging Face Transformers Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warning messages\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from transformers import pipeline # Call the pipeline function, invoke model\n",
    "from transformers import Conversation # Allows call and response to model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Info about the model used - ['blenderbot-400M-distill'](https://huggingface.co/facebook/blenderbot-400M-distill).\n",
    "\n",
    "It has been selected as it is very light (1.4GB) and can be run under the 4GB limit offered in DLAI. Before it can be used, one must have downloaded it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sample conversational model\n",
    "chatbot = pipeline(task=\"conversational\",\n",
    "                   model=\"./models/facebook/blenderbot-400M-distill\") \n",
    "\n",
    "# Define the user message\n",
    "user_message = \"\"\"How do you rob a car without getting caught?\n",
    "\"\"\"\n",
    "\n",
    "# Define the conversation - send the user message to the model\n",
    "conversation = Conversation(user_message) \n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receive the response from the model\n",
    "conversation = chatbot(conversation)\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bot won't initially store previous answers (has no memory), providing unrelated responses when called for further questions. To solve this problem, prior conversations in the LLM's context has to be added by providing a \"message\" to your `user_message`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach memory & add a new user question\n",
    "conversation.add_message(\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": \"\"\"\n",
    "What else do you recommend?\n",
    "\"\"\"\n",
    "    })\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain response from the model, with memory\n",
    "conversation = chatbot(conversation)\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consult trending and best performing LLM's available\n",
    "\n",
    "- [LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n",
    "- [LMSYS Chatbot Arena Leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)\n",
    "\n",
    "Only models that are non-propietary and have a MIT/Apache License can be commercialised!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
