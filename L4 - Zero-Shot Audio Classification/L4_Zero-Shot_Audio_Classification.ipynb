{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Audio Classification\n",
    "\n",
    "In the example provided, the libraries are already installed. If we wanted to run this, we would have to download the model libraries that we want from Hugging Face & run with the Transformers library.\n",
    "\n",
    "Some use cases and services that can be built using these models are:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install soundfile\n",
    "# !pip install librosa\n",
    "\n",
    "# The `librosa` library is used for audio processing & needs ffmepg\n",
    "# https://pypi.org/project/librosa/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset of audio recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warning messages\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "# This dataset is a collection of different environmental sounds of 5 seconds\n",
    "# dataset = load_dataset(\"ashraq/esc50\",\n",
    "#                       split=\"train[0:10]\")\n",
    "dataset = load_from_disk(\"./models/ashraq/esc50/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a random sample and see what it looks like\n",
    "audio_sample = dataset[0]\n",
    "audio_sample    # This is a dictionary with keys 'audio' and 'label'\n",
    "\n",
    "# Play the audio\n",
    "from IPython.display import Audio as IPythonAudio\n",
    "IPythonAudio(audio_sample[\"audio\"][\"array\"],\n",
    "             rate=audio_sample[\"audio\"][\"sampling_rate\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the `audio classification` pipeline using Transformers Library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Info about the model used: [clap-htsat-unfused](https://huggingface.co/laion/clap-htsat-unfused).\n",
    "\n",
    "Before it can be used, one must have downloaded it. Other models that can be used can be found by searching `clap` in Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "zero_shot_classifier = pipeline(\n",
    "    task=\"zero-shot-audio-classification\",\n",
    "    model=\"./models/laion/clap-htsat-unfused\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sampling Rate for Transformer Models\n",
    "\n",
    "Depending on the sampling rate (Hz) a model is trained with and the audio resolution of the samples, the length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sampling rate of model\n",
    "zero_shot_classifier.feature_extractor.sampling_rate\n",
    "\n",
    "# Extract sampling rate of audio\n",
    "audio_sample[\"audio\"][\"sampling_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the correct sampling rate for the input audio\n",
    "from datasets import Audio\n",
    "\n",
    "dataset = dataset.cast_column(\n",
    "    \"audio\",\n",
    "     Audio(sampling_rate=48_000))\n",
    "\n",
    "# Take an example from the dataset to check sampling rate is changed\n",
    "audio_sample = dataset[0]\n",
    "audio_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass candidate labels\n",
    "candidate_labels = [\"Sound of a dog\",\n",
    "                    \"Sound of vacuum cleaner\",\n",
    "                    \"Sound of a car\",\n",
    "                    \"Sound of a cat\",]\n",
    "\n",
    "# Classify the audio\n",
    "zero_shot_classifier(audio_sample[\"audio\"][\"array\"],\n",
    "                     candidate_labels=candidate_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
